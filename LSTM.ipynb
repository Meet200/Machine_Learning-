{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meet200/Machine_Learning-/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFEDXOilNGbO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "df6390dd-50b4-4041-f94e-1bdcb8db4b8b"
      },
      "source": [
        "from keras import layers\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import optimizers\n",
        "import numpy.random as nr\n",
        "from tensorflow import set_random_seed\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-36547eda7d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'set_random_seed' from 'tensorflow' (/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV45mq5mNy-9"
      },
      "source": [
        "\n",
        "max_features = 10000\n",
        "max_len = 250\n",
        "\n",
        "old = np.load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xJF8We2N2wl"
      },
      "source": [
        "np.load = lambda *a,**k: old(*a,**k)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2HzIDhJN5LH"
      },
      "source": [
        "(train_text, train_labels), (test_text, test_labels) = imdb.load_data(num_words = max_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emz68Yw-N6xS"
      },
      "source": [
        "np.load = old\n",
        "del(old)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHdIMEb-OH4F"
      },
      "source": [
        "train_text = preprocessing.sequence.pad_sequences(train_text, maxlen = max_len)\n",
        "test_text = preprocessing.sequence.pad_sequences(test_text, maxlen = max_len)\n",
        "print(len(train_text))\n",
        "print(train_text[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK9RgnjwOKm-"
      },
      "source": [
        "\n",
        "embedding = Sequential()\n",
        "## First add an embedding layer\n",
        "embedding.add(Embedding(10000, 8, input_length = max_len, embeddings_regularizer = regularizers.l2(0.01)))\n",
        "## Flatten the embedding of the features\n",
        "embedding.add(Flatten())\n",
        "## Now the  binary classifier layer\n",
        "embedding.add(Dense(1, activation = 'sigmoid'))\n",
        "embedding.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "embedding.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eNzvbzdOSfw"
      },
      "source": [
        "\n",
        "nr.seed(3421)\n",
        "set_random_seed(654)\n",
        "historyEMB = embedding.fit(train_text, train_labels,\n",
        "                   epochs = 20,\n",
        "                   batch_size = 256,\n",
        "                   validation_data = (test_text, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KQGzSGcOd0f"
      },
      "source": [
        "def plot_loss(history):\n",
        "    '''Function to plot the loss vs. epoch'''\n",
        "    train_loss = history.history['loss']\n",
        "    test_loss = history.history['val_loss']\n",
        "    x = list(range(1, len(test_loss) + 1))\n",
        "    plt.plot(x, test_loss, color = 'red', label = 'Test loss')\n",
        "    plt.plot(x, train_loss, label = 'Training loss')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss vs. Epoch')\n",
        "    \n",
        "plot_loss(historyEMB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Zd67gtOheR"
      },
      "source": [
        "def plot_accuracy(history):\n",
        "    train_acc = history.history['acc']\n",
        "    test_acc = history.history['val_acc']\n",
        "    x = list(range(1, len(test_acc) + 1))\n",
        "    plt.plot(x, test_acc, color = 'red', label = 'Test accuracy')\n",
        "    plt.plot(x, train_acc, label = 'Training accuracy')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Epoch')  \n",
        "    \n",
        "plot_accuracy(historyEMB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nidfqr-WO3-X"
      },
      "source": [
        "RNN1 = Sequential()\n",
        "## First add an embedding layer\n",
        "RNN1.add(Embedding(max_features, 32, embeddings_regularizer = regularizers.l2(0.01)))\n",
        "## Now add an RNN layer\n",
        "RNN1.add(SimpleRNN(32, kernel_regularizer = regularizers.l2(0.01)))\n",
        "## And the classifier layer\n",
        "RNN1.add(Dense(1, activation = 'sigmoid'))\n",
        "RNN1.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "RNN1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOWJD53OO6Mw"
      },
      "source": [
        "nr.seed(6754)\n",
        "set_random_seed(7766)\n",
        "historyRNN = RNN1.fit(train_text, train_labels,\n",
        "                   epochs = 20,\n",
        "                   batch_size = 1024,\n",
        "                   validation_data = (test_text, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPh1cOkqO8Dl"
      },
      "source": [
        "plot_loss(historyRNN)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBr1mygxO9_v"
      },
      "source": [
        "plot_accuracy(historyRNN)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-MeHCvaO_iG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJrtghxzP57D"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVibQsWtP6i_"
      },
      "source": [
        "from google.colab import drive #toimport google drive data\n",
        "drive.mount('/content/drive') #to mount the drive data into colab clound and enterr the activation code\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91E0a1kxQXcx"
      },
      "source": [
        "cd /content/drive/\"My Drive\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5M83kGUQYzA"
      },
      "source": [
        "apple_training_complete=pd.read_csv(\"NG Machine Learning/yahoofinance.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6ewX4sDQoq5"
      },
      "source": [
        "apple_training_processed = apple_training_complete.iloc[:, 1:2].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKuwPW3eQqIc"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "apple_training_scaled = scaler.fit_transform(apple_training_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVe_FDSGQsEF"
      },
      "source": [
        "features_set = []\n",
        "labels = []\n",
        "for i in range(60, 250):\n",
        "    features_set.append(apple_training_scaled[i-60:i, 0])\n",
        "    labels.append(apple_training_scaled[i, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgNRpTeHQuZA"
      },
      "source": [
        "features_set, labels = np.array(features_set), np.array(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fttjmqE7QzQq"
      },
      "source": [
        "features_set = np.reshape(features_set, (features_set.shape[0], features_set.shape[1], 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48y3YreCQ0el"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb-cD4TlQ2pi"
      },
      "source": [
        "model = Sequential()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVTH8OmsQ38p"
      },
      "source": [
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(features_set.shape[1], 1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LIU4rs6Q6sy"
      },
      "source": [
        "model.add(Dropout(0.2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0udP3b0zQ_Rq"
      },
      "source": [
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dropout(0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h819OecYRA_t"
      },
      "source": [
        "model.add(Dense(units = 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BnG63MrRDEv"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh6uwBp2REtk"
      },
      "source": [
        "model.fit(features_set, labels, epochs = 100, batch_size = 32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWtDmSc4RGOd"
      },
      "source": [
        "apple_testing_processed = apple_training_complete.iloc[:, 1:2].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvL3b_U4RR7J"
      },
      "source": [
        "apple_total = pd.concat((apple_training_complete['Open'], apple_training_complete['Open']), axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqnk0i_2c61g"
      },
      "source": [
        "test_inputs = apple_total[len(apple_total) - len(apple_training_complete) - 60:].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_ccwMrpe3gw"
      },
      "source": [
        "test_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqKf3jCVe4xV"
      },
      "source": [
        "test_inputs = test_inputs.reshape(-1,1)\n",
        "test_inputs = scaler.transform(test_inputs)\n",
        "test_features = []\n",
        "for i in range(60,250):\n",
        "    test_features.append(test_inputs[i-60:i, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd9b-DdPfECg"
      },
      "source": [
        "test_features = np.array(test_features)\n",
        "test_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xihDnybDfF6g"
      },
      "source": [
        "predictions = model.predict(test_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qvs_0S_fHEX"
      },
      "source": [
        "predictions = scaler.inverse_transform(predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhM0xR5ifISy"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(apple_testing_processed, color='blue', label='Actual Apple Stock Price')\n",
        "plt.plot(predictions , color='red', label='Predicted Apple Stock Price')\n",
        "plt.title('Apple Stock Price Prediction')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Apple Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdqf4HrjfJ8k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emE8A3N7L-Yv"
      },
      "source": [
        "from urllib.request import urlopen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB6yVRwOLgjb"
      },
      "source": [
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "file = urlopen(url)\n",
        "document=(file.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCp3omzgLlZI"
      },
      "source": [
        "document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZiBQSSTMOIV"
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkCS_H_iMmV1"
      },
      "source": [
        "document=document.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szkf9o89NXDL"
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(document)))             #set of all distinct characters\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pALNH47iI61h"
      },
      "source": [
        "list(set(document))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y1kFta-NosD"
      },
      "source": [
        "n_chars = len(document)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvloititNtxW"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = document[i:i + seq_length]\n",
        "\tseq_out = document[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR3SJ7mbNw-T"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAzP_BrUOkw5"
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksLbh1e_Oph8"
      },
      "source": [
        "model.fit(X, y, epochs=10, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0_sJ3a8O6Rg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4Ftjtm8gX4d"
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9HICQ97gZ8s"
      },
      "source": [
        "filename = \"NG Machine Learning/alice.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAaVDCLVgnG4"
      },
      "source": [
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyO1DXiUgtcC"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPkvb6E3gxyE"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rhuIJs6hBs2"
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbZ9gHWmhF5m"
      },
      "source": [
        "model.fit(X, y, epochs=10, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmLufBsrhJDW"
      },
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-10-2.1507.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWk_FWwMqM2u"
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe6ZQXQYqSVx"
      },
      "source": [
        "# pick a random seed\n",
        "generate=\"\"\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  print(result)\n",
        "  generate+=result\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print(generate)\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu92nL9bsYhr"
      },
      "source": [
        "generate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVe-tpA7uWh-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}